import torch
import copy
import pandas as pd
from TARN_MAPPO.enviroment_rolling import EnvConfig, Stock_Env
import numpy as np
from TARN_MAPPO.agent import MAPPO, EarlyStopping
import copy
from tqdm import tqdm
import wandb
import time
import torch
import numpy as np


# class EarlyStopping:
#     def __init__(self, patience=10, min_delta=0.0001):
#         """
#         PPO í•™ìŠµì—ì„œ Early Stoppingì„ ìˆ˜í–‰í•˜ëŠ” í´ë˜ìŠ¤

#         Args:
#             patience (int): íŠ¹ì • step ë™ì•ˆ value lossê°€ ê°œì„ ë˜ì§€ ì•Šìœ¼ë©´ í•™ìŠµ ì¤‘ë‹¨
#             min_delta (float): ì†ì‹¤ì´ ê°œì„ ë˜ì—ˆë‹¤ê³  íŒë‹¨í•  ìµœì†Œ ë³€í™”ëŸ‰
#         """
#         self.patience = patience
#         self.min_delta = min_delta
#         self.best_loss = float('inf')  # ì´ˆê¸°ì—ëŠ” ë§¤ìš° í° ê°’
#         self.no_improvement_steps = 0
#         self.early_stop = False

#     def check_stop(self, value_loss):
#         """
#         í˜„ì¬ stepì˜ value lossë¥¼ í™•ì¸í•˜ê³  Early Stopping ì—¬ë¶€ë¥¼ íŒë‹¨

#         Args:
#             value_loss (float): í˜„ì¬ Value Loss

#         Returns:
#             bool: Early Stopping ì—¬ë¶€ (Trueë©´ í•™ìŠµ ì¤‘ë‹¨)
#         """
#         # âœ… Value Lossê°€ ê°œì„ ë˜ì—ˆì„ ê²½ìš°
#         if value_loss < self.best_loss - self.min_delta:
#             self.best_loss = value_loss
#             self.no_improvement_steps = 0  # ê°œì„ ë˜ì—ˆìœ¼ë¯€ë¡œ ì´ˆê¸°í™”
#         else:
#             self.no_improvement_steps += 1  # ê°œì„ ë˜ì§€ ì•Šìœ¼ë©´ ì¦ê°€

#         # âœ… íŠ¹ì • Step ë™ì•ˆ ê°œì„ ì´ ì—†ìœ¼ë©´ Early Stopping í™œì„±í™”
#         if self.no_improvement_steps >= self.patience:
#             print(f"Early Stopping Triggered: {self.patience} Step ë™ì•ˆ Value Loss ê°œì„  ì—†ìŒ")
#             self.early_stop = True

#         return self.early_stop

class EarlyStopping:
    def __init__(self, patience=10, min_delta=0.001):
        self.patience = patience
        self.min_delta = min_delta
        self.best_score = float('-inf')
        self.no_improvement_steps = 0
        self.early_stop = False

    def check_stop(self, current_score):
        if current_score > self.best_score + self.min_delta:
            self.best_score = current_score
            self.no_improvement_steps = 0
        else:
            self.no_improvement_steps += 1

        if self.no_improvement_steps >= self.patience:
            self.early_stop = True
            print(f"Early Stopping Triggered: {self.patience} Step ë™ì•ˆ ê°œì„  ì—†ìŒ")

        return self.early_stop


def train_daily_env(train_dataset, config):
    """
    Main function to train and evaluate the competitive MAPPO agents.

    Args:
        env_config: Configuration for the Stock_Env.
        num_episodes (int): Number of training episodes.

    Returns:
        None
    """

    # Initialize environment
    env = Stock_Env(config)
    # early_stopping_reward = EarlyStopping(patience=config.patience, min_delta=0.0001)

    print("í•™ìŠµ í™˜ê²½ ë° ì—ì´ì „íŠ¸ ìƒì„± ì™„ë£Œ")
    print("í•™ìŠµ ì‹œì‘")
    print("í•™ìŠµ lr_actor ìˆ˜:", env.lr_actor)
    print("í•™ìŠµ lr_critic ê¸¸ì´:", env.lr_critic)
    print("í•™ìŠµ entropy_weight ìˆ˜:", env.entropy_weight)
    print("í•™ìŠµ K_epochs ìˆ˜:", env.K_epochs)


    # Initialize agents
    agent = MAPPO(env)

    # Optimizers for each agent
# Optimizers for each agent
    # optimizer = {
    #         "actor_optimizer": torch.optim.Adam(
    #             [param for actor in agent.actors for param in actor.parameters()],
    #             lr=env.lr_actor,
    #         ),
    #         "critic_optimizer": torch.optim.Adam(
    #             list(agent.critic.parameters()),
    #             lr=env.lr_critic,
    #         ),
    #     }


    # Training loop
    # for epoch in tqdm(range(config.epochs + 1)):

# ë§¤ epochë§ˆë‹¤ ë©”ì¸ ì—í”¼ì†Œë“œë“¤ ëŒë©´ì„œ í•™ìŠµ
    time_step = 0
    logging_step = 1
    early_stopping = EarlyStopping(patience=10, min_delta=0.0001)

    while time_step < env.max_step:
        state = env.reset()
        # global_time_step = 0  # ì „ì²´ íƒ€ì„ìŠ¤í… ì¹´ìš´íŠ¸
        # for main_episode in range(2):                  
        # state = env.window_slice() # ìŠ¤í…Œì´íŠ¸ ì§¤ë¦¼, dayì§¤ë¦¼, max_step ì¡°ì •ë¨.
        state = state.permute(1, 2, 0)
        # state = torch.tensor(state, dtype=torch.float32)
                    
        total_sub_rewards = [[] for _ in range(env.num_agents)]
        # sub_epoch_rewards = [0] * env.num_agents
        for day_step in range(env.update_interval):
            actions = agent.select_actions(state)
            actions = torch.tensor(actions).cpu().numpy()

            next_state, rewards, done, invest_money_list, reward_dict = env.step(actions)
            rewards = torch.tensor(rewards, dtype=torch.float32).to(config.device)
            
            agent.store_reward(rewards, [done] * env.num_agents)
            next_state = next_state.permute(1, 2, 0)
            state = torch.tensor(next_state, dtype=torch.float32)
            # for reward in rewards:
            #     if reward > 20:
            #         print(f"reward: {reward}")
            # sub_epoch_reward += reward.item()
            for i in range(env.num_agents):
                # sub_epoch_rewards[i] += rewards[i].item()
                total_sub_rewards[i].append(rewards[i].item())
                # wandb.log({
                #     f"agent_{i}_sub_episode_reward": rewards[i].item(),
                #     "time_step": global_time_step})
            # sub_epoch_reward += np.mean(rewards)

            # total_sub_reward.append(sub_epoch_reward) # ì„œë¸Œì—í”¼ì†Œë“œ ë³„ ë³´ìƒ ê¸°ë¡
            # 6) ì¼ì • ìŠ¤í…ë§ˆë‹¤ PPO ì—…ë°ì´íŠ¸
            # if time_step % config.update_interval == 0:
            #     agent.update()
            #     time_step = 0

            # if done:
            #     break
            time_step += 1

            if time_step % env.update_interval == 0:
                
                torch.set_grad_enabled(True)
                agent_returns, best_agent_loss =  agent.all_update(logging_step)
                torch.set_grad_enabled(False)
                logging_step += 1
                # # ê° ì—ì´ì „íŠ¸ì˜ ì†ì‹¤ ê¸°ë¡
                # for i in range(env.num_agents):
                    
                #     last_metrics = agent.agents[i].get_last_metrics()
                #     wandb.log({
                #         **{f"agent_{i}_{key}_per_sub_episode": value for key, value in last_metrics.items()},
                #         "time_step": update_step
                #     })
                # update_step += 1
                # âœ… Early Stopping ì²´í¬
                if early_stopping.check_stop(best_agent_loss):
                    print(f"ğŸ† Early Stopping Triggered at step {time_step}")
                    wandb.log({"Early_Stopping": time_step})
                    wandb.log({
                        "Best_Agent_Avg_Reward": best_agent_loss,
                        "Step": time_step
                    })
                    agent.save()

                    return agent  # í•™ìŠµ ì¤‘ë‹¨

            # ì£¼ê¸°ì ìœ¼ë¡œ ìµœê³  ì„±ëŠ¥ ì—ì´ì „íŠ¸ ëª¨ë¸ ì €ì¥
            if time_step % env.update_interval == 0:
            # if epoch % 1 == 0:

                agent.save()
                print(f"Best agent model saved at time_step {time_step}")
            
            if done:
                break
            
        # main_episode_rewards.append(agent_returns)
        # main_episode_rewards.append(total_sub_reward)


        # ì „ì²´ ì—ì´ì „íŠ¸ì˜ í‰ê·  ë³´ìƒ ê³„ì‚°

        # avg_main_episode_rewards = []
        # for agent_idx in range(env.num_agents):
        #     # mian_episode_rewards[list] : [[sub_episode1_agent1, sub_episode2_agent1, ...], [sub_episode1_agent2, sub_episode2_agent2, ...], ...]
        #     # ì—ì´ì „íŠ¸ ë³„ ì„œë¸Œ ì—í”¼ì†Œë“œì˜ reward
        #     agent_rewards = [episode[agent_idx] for episode in main_episode_rewards] 
        #     avg_reward = np.mean(agent_rewards)
        #     avg_main_episode_rewards.append(avg_reward)

        #     wandb.log({
        #         f"agent_{agent_idx}_main_episode_reward": avg_reward,
        #         "epoch": epoch
        #     })

        
        # ìµœê³  ì„±ëŠ¥ ì—ì´ì „íŠ¸ì˜ ë³´ìƒìœ¼ë¡œ early stopping ì²´í¬
        # best_reward = max(agent_returns) # ì—ì´ì „íŠ¸ ì¤‘ ê°€ì¥ ë†’ì€ ë³´ìƒ
        # early_stopping_reward(best_reward)

            
        # avg_main_episode_reward = np.mean([np.mean(x) for x in main_episode_rewards]) # ì „ì²´ ì—ì´ì „íŠ¸ì˜ í‰ê·  ë³´ìƒ
        # wandb.log({"main_episode_reward": avg_main_episode_reward, "epoch": epoch})
        
        # early_stopping_reward(avg_main_episode_reward)
        
        # if early_stopping_reward.early_stop:
        #     print(f"Early stopping triggered by reward at epoch {epoch}. "
        #           f"Best Reward: {best_reward:.2f}")
        #     break

        # # ì£¼ê¸°ì ìœ¼ë¡œ ë¡œê·¸ ì¶œë ¥
        # if epoch % config.log_interval == 0:
        #     print(f"Epoch {epoch}/{config.epochs}")
        #     for i, avg_reward in enumerate(avg_main_episode_rewards):
        #         print(f"Agent {i} Average Reward: {avg_reward:.2f}")
        #     print(f"Best Agent: {agent.best_agent_idx}, Best Reward: {best_reward:.2f}")

        # ì£¼ê¸°ì ìœ¼ë¡œ ìµœê³  ì„±ëŠ¥ ì—ì´ì „íŠ¸ ëª¨ë¸ ì €ì¥
        if time_step % 200 == 0:
        # if epoch % 1 == 0:

            agent.save()
            print(f"Best agent model saved at time_step {time_step}")
    agent.save()
    
    return agent


def train_competitive_ippo(env, agents, config):
    """
    CompetitiveMultiAgentEnvì—ì„œ,
    n_agentsê°œì˜ ë…ë¦½ PPO(agents)ë¥¼ ë™ì‹œì— í•™ìŠµ.
    """
    n_agents = config.num_agents
    max_episodes = config.epochs  # ì˜ˆ: ì—í­=ì—í”¼ì†Œë“œ
    rollout_length = 100  # ì˜ˆ: í•œ ì—í”¼ì†Œë“œ or fixed step ìˆ˜

    for episode in range(max_episodes):
        obs = env.reset()  # shape [n_agents, obs_dim]
        done = np.array([False]*n_agents)
        step_count = 0

        # ê° ì—ì´ì „íŠ¸ buffer clear
        for i in range(n_agents):
            agents[i].buffer.clear()

        while not done.any() and step_count < rollout_length:
            actions = []
            for i in range(n_agents):
                # obs[i] shape [obs_dim]
                obs_i = obs[i].cpu().numpy()  
                action_i = agents[i].select_action(obs_i)  # store in agent buffer
                actions.append(action_i)

            # actions shape => [n_agents, action_dim]
            actions_t = torch.tensor(actions, dtype=torch.float32)

            next_obs, rewards, dones, info = env.step(actions_t)
            # store reward/done in each agent's buffer
            for i in range(n_agents):
                agents[i].store_reward_done(rewards[i], dones[i])

            obs = next_obs
            done = dones
            step_count += 1

        # í•œ ì—í”¼ì†Œë“œê°€ ëë‚˜ê±°ë‚˜ rollout_length ë„ë‹¬ => PPO update
        for i in range(n_agents):
            agents[i].update()

        # logging
        print(f"Episode {episode} done.")
    return agents
    

def eval_env(env, agent):
    
    """
    í…ŒìŠ¤íŠ¸ ê¸°ê°„ë™ì•ˆ í‰ê°€í•˜ê¸° ìœ„í•œ ì½”ë“œì…ë‹ˆë‹¤.
    """
    # print("í‰ê°€ í™˜ê²½ ìƒì„±")
    # config = EnvConfig(test_tensor, test_dataset)
    # env = copy.deepcopy(Stock_Env(test_tensor, test_dataset, config))
    # print("í‰ê°€ ì‹œì‘")
    state = env.reset()
    state = torch.tensor(state, dtype=torch.float32).to(env.device)
    rewards = []
    asset_weight_li = []
    top3_action_li = []
    agent.policy.eval()         # í‰ê°€ ëª¨ë“œ ì „í™˜
    agent.policy_old.eval()     # (í•„ìš”í•˜ë‹¤ë©´) old policyë„ í‰ê°€ ëª¨ë“œë¡œ
    attn_li = []
    empty_df = pd.DataFrame()

    all_weight_df = pd.DataFrame()
    
    asset_weight_df = pd.DataFrame()
    
    for _ in range(env.max_step):
        state = state.permute(1, 2, 0)
        state = torch.tensor(state, dtype=torch.float32).to(env.device)
        with torch.no_grad():
            action, attn_score = agent.select_action(state,  deterministic=True)
            attn_li.append(attn_score)
        next_state, rewards, done, n_rewards,  all_weight_invest_rewards, asset_weight_dict, top3_action, empty_df, all_weight_df, asset_weight_df = env.eval_step(action, empty_df, all_weight_df, asset_weight_df)
        asset_weight_li.append(asset_weight_dict)
        top3_action_li.append(top3_action)
        state = torch.tensor(next_state, dtype=torch.float32).to(env.device)
        # rewards.append(reward_dict["return"])
        if done:
            break
        
    return rewards, n_rewards, all_weight_invest_rewards, asset_weight_li, top3_action_li, attn_li, empty_df, all_weight_df, asset_weight_df


if __name__ == "__main__":
    import torch
    import copy
    import pandas as pd
    from enviroment import EnvConfig, Stock_Env
    import numpy as np
    from agent import PPO
    import pickle
    # pickle íŒŒì¼ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ë°©ë²•
    with open("./data/train_real_data.pkl", "rb") as f:
        train_dataset_episode = pickle.load(f)
    
    
    train_dataset = pd.read_csv("./data/train_data.csv", index_col=0)

    
    feature_dim = 35


    train_scaled_tensor = torch.load("./data/all_episodes.pt")
    
    # --- Hyperparameters ---
    def get_hyperparameters(feature_dim):
        return {
            "dcc_dropout": 0.2,
            "sac_dropout": 0.01,
            "sac_heads": 2,
            "ddc_configs": [
                {"in_channels": feature_dim, "out_channels": 8, "kernel_size": 3, "stride": 1, "padding": (3-1)//2, "dilation": 1, "sac_scale": 4**0.5, "residual_out_channels" : 8, "residual_kernal": 1},
                {"in_channels": 8, "out_channels": 16, "kernel_size": 3, "stride": 1, "padding": 2, "dilation": 2, "sac_scale": 8**0.5, "residual_out_channels" : 16, "residual_kernal": 1 },
                {"in_channels": 16, "out_channels": 16, "kernel_size": 3, "stride": 1, "padding": 4, "dilation": 4, "sac_scale": 8**0.5}
            ],
            "final_conv_config": {"in_channels" : 16, "out_channels": 8, "kernel_size": 31, "stride": 1, "padding": 0}
        }

    hyperparameters = get_hyperparameters(feature_dim)

    

    train_agent = train_main_sub_env(train_scaled_tensor, train_dataset, train_dataset_episode, hyperparameters)
    
    # rewards = eval_env(test_tensor, test_dataset, train_agent)
